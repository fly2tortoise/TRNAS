#####################################################
# Copyright (c) Xuanyi Dong [GitHub D-X-Y], 2019.08 #
#####################################################
import time, torch
import sys

class loss_cure():
    def __init__(self, net, criterion, lambda_, device='cuda'):
        self.net = net
        self.criterion = criterion
        self.lambda_ = lambda_  # 1
        self.device = device

    def _find_z(self, inputs, targets, h):
        # è¯¥å‡½æ•°ä¸»è¦è®¡ç®—ä¸€ä¸ªå½’ä¸€åŒ–çš„æ‰°åŠ¨å‘é‡ ð‘§, ç”¨äºŽå¯¹è¾“å…¥æ•°æ®æ–½åŠ æ‰°åŠ¨, å¹¶è¿”å›žå…¶èŒƒæ•° norm_grad, é€šå¸¸ç”¨äºŽè¯„ä¼°æ¨¡åž‹çš„é²æ£’æ€§ï¼ˆä¾‹å¦‚å¯¹æŠ—è®­ç»ƒæˆ–é²æ£’ä¼˜åŒ–ï¼‰
        # print("inputs.size: ",inputs.size()) # torch.Size([8, 3, 32, 32])

        inputs.requires_grad_()
        outputs = self.net.eval()(inputs)  # ä¸€ä¸ªå®Œæ•´çš„åˆ†ç±»æ¨¡åž‹çš„è¾“å…¥å’Œè¾“å‡º
        loss_z = self.criterion(outputs, targets) # self.net.eval()(inputs)
        loss_z.backward()                  # torch.ones(targets.size(), dtype=torch.float).to(self.device)
        grad = inputs.grad.data + 0.0      # å½¢çŠ¶ç›¸åŒçš„å¼ é‡
        norm_grad = grad.norm().item()     # è®¡ç®—L2èŒƒæ•°
        z = torch.sign(grad).detach() + 0. # zæ˜¯è‡ªæˆ‘æž„é€ çš„å½’ä¸€åŒ–æ‰°åŠ¨ ï¼ˆä½¿ç”¨è¾“å…¥çš„æ¢¯åº¦ä¿¡æ¯ç”Ÿæˆæ‰°åŠ¨æ–¹å‘ï¼Œç±»ä¼¼FGSMï¼‰
        z = 1. * (h) * (z + 1e-7) / (z.reshape(z.size(0), -1).norm(dim=1)[:, None, None, None] + 1e-7)
        inputs.grad.detach()
        inputs.grad.zero_()
        # zero_gradients(inputs)
        self.net.zero_grad()

        return z, norm_grad

    def regularizer(self, inputs, targets, h=3., lambda_=4):
        '''
        Regularizer term in CUREï¼š å®ƒé€šè¿‡å¯¹æ¯”æ‰°åŠ¨å‰åŽçš„æŸå¤±å·®å¼‚åŠå…¶å¯¹è¾“å…¥çš„æ¢¯åº¦å½±å“ï¼Œè®¡ç®—å‡ºä¸€ä¸ªæ­£åˆ™åŒ–å€¼ã€‚
        '''
        # ç”Ÿæˆä¸€ä¸ªåŸºäºŽæ¢¯åº¦æ–¹å‘çš„æ‰°åŠ¨zï¼Œç”Ÿæˆå¹…åº¦ h
        z, norm_grad = self._find_z(inputs, targets, h)
        # print("regularizer: ", z.size(), norm_grad)  # æ‰°åŠ¨ + æ¢¯åº¦çš„L2èŒƒæ•°

        inputs.requires_grad_()
        outputs_pos = self.net.eval()(inputs + z)
        outputs_orig = self.net.eval()(inputs)

        loss_pos = self.criterion(outputs_pos, targets)
        loss_orig = self.criterion(outputs_orig, targets)

        # ä¸¤ä¸ªç›¸åŒæ ·æœ¬, ä½†æ˜¯ä¸€ä¸ªæ”»å‡»åŽloss + æ²¡æ”»å‡»çš„loss, è¿”å›žä¸¤è€…ä¹‹é—´çš„æ¢¯åº¦å·®è·
        grad_diff = torch.autograd.grad((loss_pos - loss_orig), inputs)[0] # torch.Size([8, 3, 32, 32])
        reg = grad_diff.reshape(grad_diff.size(0), -1).norm(dim=1)         # reg: æ¯ä¸ªæ ·æœ¬æ¢¯åº¦å·®è·çš„L2èŒƒæ•° # 8ä¸ªæ ·æœ¬
        self.net.zero_grad()

        return torch.sum(self.lambda_ * reg) / float(inputs.size(0)), norm_grad  # æ¯ä¸ªæ ·æœ¬çš„å¹³å‡æ¢¯åº¦å·®å€¼


def procedure(train_loader_1, train_loader_2, network, criterion, scheduler, optimizer, mode, grad=False, h=3.0):
    # mode é»˜è®¤æ˜¯ train
  # losses, top1, top5 = AverageMeter(), AverageMeter(), AverageMeter()
  if mode == 'train'  : network.train()
  elif mode == 'valid': network.eval()
  else: raise ValueError("The mode is not right : {:}".format(mode))
  grads = {}
  # data_time, batch_time, end = AverageMeter(), AverageMeter(), time.time()

  ############################################# adjust h
  # loader2 ç”¨äºŽæ­£åˆ™åŒ–è°ƒæ•´
  inputs, targets = next(iter(train_loader_2)) # torch.Size([8, 3, 32, 32])
  inputs = inputs.cuda()
  targets = targets.cuda(non_blocking=True)
  reg = loss_cure(network, criterion, lambda_=1, device='cuda')
  regularizer_average, grad_norm = reg.regularizer(inputs, targets, h = h)
  # print("regularizer_average: ",regularizer_average)  # tensor(4.2430e-05, device='cuda:0')   æ¯ä¸ªæ ·æœ¬ï¼Œè¢«æ”»å‡»ä¹‹åŽçš„æ¨¡åž‹è¾“å‡ºå¹³å‡å˜åŒ–å€¼

  # 50 æ¬¡ æ˜¯å†…éƒ¨å¾ªçŽ¯æ‰€æŽ§åˆ¶ï¼Œç”¨äºŽè®¡ç®—ç‰¹å¾å€¼
  for i, (inputs, targets) in enumerate(train_loader_1):
    # print(inputs.size())
    inputs = inputs.cuda()
    targets = targets.cuda(non_blocking=True)
    if mode != 'train': return 0,0,0,time.time()-time.time()

    logits = network(inputs)
    loss   = criterion(logits, targets)
    # backward
    if mode == 'train':
      loss.backward()
      import copy
      index_grad = 0
      index_name = 0

      # æ¢¯åº¦çš„æå–ä¸Žå­˜å‚¨  å†…éƒ¨10æ¬¡å¾ªçŽ¯
      for name, param in network.named_parameters():
           # print(name) # ä¼¼ä¹Žåªè®¡ç®—äº† DARTS cell 0 çš„å‰ä¸¤æ¡è¾¹
           if param.grad is None:
                print('param.grad is None')
                print(name)
                continue
           #if param.grad.view(-1)[0] == 0 and param.grad.view(-1)[1] == 0: continue #print(name)
           if index_name > 10: break
           if len(param.grad.view(-1).data[0:100]) < 50: # å°†æ¯ä¸ªæ¢¯åº¦å¹³é“ºä¸ºä¸€ç»´ï¼Œ æå–å‰ 100 ä¸ªå…ƒç´ å¹¶å­˜å‚¨åˆ°gradå­—å…¸ä¸­ (ä¿å­˜10ä¸ªæ¢¯åº¦lenä¸º50çš„æ¨¡åž‹æ•°æ®)
               continue
           index_grad = name
           index_name += 1

           if name in grads: # æ”¶é›†è®¡ç®—çš„å‚æ•°
               grads[name].append(copy.copy(param.grad.view(-1).data[0:100])) # å·²å­˜å‚¨åˆ™è¿½åŠ 
           else:
               grads[name]=[copy.copy(param.grad.view(-1).data[0:100])]       # æ²¡å­˜å‚¨åˆ™æ–°å»º

      if len(grads[index_grad]) == 50:  # æŸä¸ªæ“ä½œçš„æ¢¯åº¦å€¼listï¼Œé•¿åº¦ =50ã€‚
             # print(index_grad)
             conv = 0
             maxconv = 0
             minconv = 0
             lower_layer = 1
             top_layer = 1
             para = 0

             for name in grads:
                # print(len(grads[name]))   # len = 50
                for i in range(50): # nt(self.grads[name][0].size()[0])):
                   #if len(grads[name])!=: print(name)
                   #for j in range(50):
                   #if i == j: continue
                   grad1 = torch.tensor([grads[name][k][i] for k in range(25)])
                   grad2 = torch.tensor([grads[name][k][i] for k in range(25,50)])
                   # grad1 = grad1 - grad1.mean()
                   # grad2 = grad2 - grad2.mean()
                   # print(grad1.size(), grad2.size()) [25], [25]
                   conv += torch.dot(grad1, grad2) / 2500
                   para += 1
             # print(para) 550
             break

    # count time
    # batch_time.update(time.time() - end)
    # end = time.time()

  if mode == 'train':
      # print(conv)
      # sys.exit()
      RF = -torch.exp(conv  * 5000000) * regularizer_average  # convåº”è¯¥å°±æ˜¯ç‰¹å¾å€¼  * tensor(4.2430e-05, device='cuda:0')  è¿™å—æœ‰ç‚¹åƒL: |y - f(x)|

      return RF#, 0,0, batch_time.sum #conv, maxconv, minconv

  else:
      return 0#,0,0, batch_time.sum

